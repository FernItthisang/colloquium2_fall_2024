---
student: Christina Huang
tags: students
image: unboxing.png
---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Unboxing Biases in AI Resume Screening</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #FFFFFF;
            color: #000;
        }
        header {
            text-align: center;
            padding: 40px 20px;
            background-color: #FFFFFF;
            border-bottom: 2px solid #000;
        }
        header h1 {
            font-size: 48px;
            font-weight: bold;
            color: #000;
            margin: 0;
        }
        header h2 {
            font-size: 20px;
            font-weight: normal;
            color: #777;
            margin-top: 10px;
        }
        section {
            padding: 40px 20px;
            margin: 0 auto;
            max-width: 1100px;
        }
        section h2 {
            font-size: 32px;
            color: #000;
            margin-bottom: 20px;
        }
        section p {
            font-size: 18px;
            line-height: 1.6;
            color: #333;
        }
        section ul {
            list-style: none;
            padding: 0;
        }
        section ul li {
            font-size: 18px;
            color: #333;
            line-height: 1.6;
        }
        section a {
            color: #1974D2;
            text-decoration: none;
        }
        section a:hover {
            text-decoration: underline;
        }
        iframe {
            width: 100%;
            height: 500px;
            border: none;
            margin-top: 40px;
        }
        footer {
            text-align: center;
            padding: 20px;
            background-color: #FFFFFF;
            font-size: 16px;
            color: #777;
            border-top: 2px solid #000;
        }
        @media (max-width: 768px) {
            body {
                font-size: 16px;
            }
            header h1 {
                font-size: 36px;
            }
            header h2 {
                font-size: 16px;
            }
            section h2 {
                font-size: 28px;
            }
            section p {
                font-size: 16px;
            }
            iframe {
                height: 350px;
            }
        }
    </style>
</head>
<body>

<header>
    <h1>Unboxing Biases in AI Resume Screening</h1>
    <h2>Christina Huang | M.S. Computational Design Practices</h2>
</header>

<section>
    <h2>Hypothesis, Research Question, or Provocation</h2>
    <p>
        Interactive visualizations of AI-powered hiring algorithms can help users identify biases in recruitment processes,
        and integrating skill-based data and algorithms into these systems can improve fairness, making hiring decisions more equitable and transparent.
    </p>
</section>

<section>
    <h2>Project Description</h2>
    <p>
        This project explores how AI-powered hiring algorithms can both improve recruitment efficiency and perpetuate bias, especially in recruitment practices. Through an interactive system map, the project visualizes the AI hiring process, highlighting the key points where biases, such as gender or racial disparities, may occur in automated decisions. The visualization allows users to interactively explore these biases, deepening their understanding of how AI algorithms score and rank resumes. Additionally, the project proposes integrating skill-based data and algorithms into the AI model, suggesting a potential solution to refocus hiring practices and increase fairness. This project emphasizes the need for careful oversight of AI hiring tools to ensure transparency, inclusivity, and equitable outcomes in real-world hiring decisions.
    </p>
</section>

<section>
    <h2>Computational Methods</h2>
    <p>
        The project combines two computational methods:
        <ul>
            <li><b>Reviewing existing research papers on biases in AI resume screening</b> to run the algorithms used in those studies with the same datasets</li>
            <li><b>Using Scrollytelling tools (e.g., Scrollama, Scrollmagic)</b> to create an interactive visualization of the AI hiring process. This approach allows users to explore how AI systems score resumes over time and see how various biases, like gender or race, affect decision-making. By using computational tools and interactive storytelling, the project helps to make hidden biases in AI algorithms transparent and showcases possible interventions, including the integration of skill-based data to refocus AI decision-making.</li>
        </ul>
    </p>
</section>

<section>
    <h2>Design Methods</h2>
    <p>
        This project adopts a critical and interactive design approach to investigate AI biases in recruitment systems. The design methods involve:
        <ul>
            <li><b>Critical Design Focus:</b> The project critiques current AI-driven recruitment practices, examining how biases are embedded in the algorithms and their effects on hiring decisions.</li>
            <li><b>Interactive Visualization:</b> Multiple interactive maps will visualize the flow of data through AI models, helping users understand how input variables (such as gender, race, and experience) influence the hiring process.</li>
            <li><b>Hands-on Experimentation:</b> Users can adjust inputs, such as gender or race, to observe the effects on AI decision-making, providing a deeper understanding of how these biases play out in practice.</li>
            <b>Actionable Insights:</b> By presenting potential interventions, the project aims to offer practical solutions to mitigate bias and promote fairness, helping AI algorithms lead to more equitable hiring practices.</li>
        </ul>
    </p>
</section>

<section>
    <h2>Precedents</h2>
    <p>
        The concept of AI-driven recruitment and its potential for bias has been explored in multiple studies, such as "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval" by [Author]. This research examines how language models used in resume screening can perpetuate gender and racial biases.
    </p>
    <ul>
        <li><a href="https://www.bloomberg.com/news/newsletters/2024-03-08/openai-s-gpt-shows-bias-in-resume-screening-experiment-big-take">OpenAI’s GPT Shows Bias in Résumé Ranking Experiment</a></li>
        <li><a href="https://anatomyof.ai/">Anatomy of an AI System</a></li>
        <li><a href="https://unboxedcity.mit.edu/">Unboxed City: Critical Explorations of AI and Cities</a></li>
        <li><a href="http://building-without-bias.co.uk/about.html">Building Without Bias</a></li>
        <li><a href="https://data-feminism.mitpress.mit.edu/">Data Feminism</a></li>
        <li><a href="https://cyberfeminismindex.com/2">Cyberfeminism Index</a></li>
    </ul>
</section>

<section>
    <h2>Proof of Concept</h2>
    <p>
        The proof of concept for this project involves demonstrating an interactive visualization of the AI-powered hiring process. The system map will visually represent how resumes are processed by AI algorithms, highlighting the points where biases may occur. Users will be able to adjust variables like gender, race, and skill sets, observing how these changes influence AI decisions. Additionally, the integration of skill-based data will be showcased, illustrating how this shift can help reduce bias and improve fairness. Prototypes, wireframes, and interactive demos will be used to visualize these concepts, ensuring that users can experience firsthand the impact of these interventions on the hiring process.
    </p>

    <!-- Figma Embed -->
    <iframe src="https://embed.figma.com/proto/lN06NjBBqglAr6uEtXGydL/Unboxing?page-id=0%3A1&node-id=1-2&node-type=frame&viewport=-690%2C-3574%2C0.39&scaling=min-zoom&content-scaling=fixed&starting-point-node-id=1%3A2&embed-host=share" allowfullscreen></iframe>
</section>

<section>
    <h2>Audience</h2>
    <p>
        The primary audience for this project includes HR professionals, recruiters, AI researchers, and diversity and inclusion advocates. By providing an interactive experience, the project aims to raise awareness about biases in AI-driven hiring systems and offer actionable insights into how they can be mitigated. The project will also appeal to job seekers who want to better understand the AI hiring processes affecting them. To make the project more inclusive, it could be adapted for different audiences, such as HR managers in smaller organizations or educational institutions, by tailoring the visualizations and insights to their specific hiring practices and needs.
    </p>
</section>

<section>
    <h2>Data</h2>
    <p>
        The data used in this project will include publicly available resume datasets, as well as AI model inputs and outputs from research studies on biases in hiring algorithms. For example, the dataset used in the research paper "Gender, Race, and Intersectional Bias in Resume Screening" will be analyzed to observe how different demographic factors influence AI decision-making:
    </p>
    <ul>
        <li><a href="https://github.com/kyrawilson/resume-screening-bias?tab=readme-ov-file">GitHub Page for Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval</a></li>
        <li><a href="https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset">Resume Dataset</a></li>
        <li><a href="https://www.kaggle.com/datasets/marcocavaco/scraped-job-descriptions">Scraped Job Descriptions</a></li>
    </ul>
</section>

<section>
    <h2>Research</h2>
    <ul>
        <li><a href="https://arxiv.org/abs/2407.20371">Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval</a></li>
        <li><a href="https://www.mdpi.com/2673-2688/5/1/19">A Comprehensive Review of AI Techniques for Addressing Algorithmic Bias in Job Hiring</a></li>
        <li><a href="https://arxiv.org/abs/2309.13933">Fairness and Bias in Algorithmic Hiring: A Multidisciplinary Survey</a></li>
        <li><a href="https://arxiv.org/abs/2405.04412">The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring</a></li>
        <li><a href="https://ir.lawnet.fordham.edu/jcfl/vol28/iss1/5/">Hired by a Machine: Can a New York City Law Enforce Algorithmic Fairness in Hiring Practices?</a></li>
    </ul>
</section>

<section>
    <h2>Next Steps</h2>
    <p>
        Incorporating Feedback from Critiques on December 10, 2024
    </p>
    <ul>
        <li><b>Explore Alternative UI Options:</b> Explore alternative UI formats like dashboards, interactive simulations, or visual tools.</li>
        <li><b>Engage Experts and Practitioners:</b> Connect with researchers and industry professionals to discuss algorithms, datasets, and methodologies.</li>
        <li><b>Understand Stakeholder Pain Points:</b> Identify challenges for decision-makers, recruiters, and candidates (i.e., key interactions, bias points, and opportunities for improvement in the AI hiring process).</li>
        <li><b>Iterate on Deliverable:</b> Refine final deliverables to align with feedback and enhance clarity and impact</li>
    </ul>
</section>

<footer>
    <p>
    Colloquium II | Fall 2024
    </p>
</footer>

</body>
</html>
